{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from matplotlib.patches import Ellipse\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torchvision\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "import shutil"
      ],
      "metadata": {
        "id": "kJERQF6-3z_T"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The idea of using a Gaussian Mixture Model for testing is to split the data into subclasses within existing classes using a Gaussian Mixture Model. This is done to deal with the problem of high intra-class variance\n",
        "and small inter-cluster dissimilarity while evaluating the quality of the generated images."
      ],
      "metadata": {
        "id": "cE3w19Uj3Vyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with zipfile.ZipFile(\"/content/minefree-class-split-wo-borders.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"data\")"
      ],
      "metadata": {
        "id": "NKUMsXqX3ZeJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_dirs = ['/content/data/minefree-class-split-wo-borders/test/bombed', '/content/data/minefree-class-split-wo-borders/train/bombed']\n",
        "target_dir = '/content/data/bombed'\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "for src in source_dirs:\n",
        "    for file_name in os.listdir(src):\n",
        "        full_file_name = os.path.join(src, file_name)\n",
        "        if os.path.isfile(full_file_name):\n",
        "            shutil.copy(full_file_name, target_dir)"
      ],
      "metadata": {
        "id": "GV_4-cYFH72e"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_dirs = ['/content/data/minefree-class-split-wo-borders/test/not bombed', '/content/data/minefree-class-split-wo-borders/train/not bombed']\n",
        "target_dir = '/content/data/not bombed'\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "for src in source_dirs:\n",
        "    for file_name in os.listdir(src):\n",
        "        full_file_name = os.path.join(src, file_name)\n",
        "        if os.path.isfile(full_file_name):\n",
        "            shutil.copy(full_file_name, target_dir)"
      ],
      "metadata": {
        "id": "gUJ4FUj_IaxU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gaussian Mixture Model"
      ],
      "metadata": {
        "id": "NKsrFucR1wyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_ellipse(mu: np.ndarray, cov: np.ndarray, ax):\n",
        "    eigenvalues, eigenvectors = np.linalg.eigh(cov)\n",
        "    angle = np.degrees(np.arctan2(*eigenvectors[:, 0][::-1]))\n",
        "    width, height = 2 * np.sqrt(5.991 * eigenvalues)  # 95% confidence interval\n",
        "    ellipse = Ellipse(xy=mu, width=width, height=height, angle=angle, fill=False)\n",
        "    ax.add_patch(ellipse)\n",
        "    ax.scatter(mu[0], mu[1], color=\"r\", marker=\"o\", label=\"Mean\")\n",
        "\n",
        "\n",
        "def plot_gmm_data(data: np.ndarray, mus, sigmas):\n",
        "    _, ax = plt.subplots()\n",
        "    for mu, sigma in zip(mus, sigmas):\n",
        "        plot_ellipse(mu, sigma, ax)\n",
        "\n",
        "    plt.scatter(data[:, 0], data[:, 1], s=5)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Y7p6OdiP5_TR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultivariateGaussian(nn.Module):\n",
        "    def __init__(self, dim: int, mu: np.ndarray = None, R: np.ndarray = None):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "        if mu is None:\n",
        "            mu = np.zeros(self.dim)\n",
        "        self.mu = nn.Parameter(torch.tensor(mu).to(torch.float32))\n",
        "\n",
        "        if R is None:\n",
        "            R = np.random.normal(size=(self.dim, self.dim))\n",
        "        self.R = nn.Parameter(torch.tensor(R).to(torch.float32))\n",
        "\n",
        "    def sigma(self) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Computes the covariance matrix of the multivariate Gaussian distribution.\n",
        "        \"\"\"\n",
        "        Sigma = self.R @ self.R.T\n",
        "        return Sigma\n",
        "\n",
        "    def sample(self) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Samples from this Gaussian.\n",
        "        \"\"\"\n",
        "        sampled_vector = self.R @ torch.randn(self.dim) + self.mu\n",
        "        return sampled_vector\n",
        "\n",
        "    def log_likelihood(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Computes the log likelihood of the multivariate Gaussian distribution at the provided point.\n",
        "        \"\"\"\n",
        "        Sigma_det = torch.det(self.sigma())\n",
        "        Sigma_inv = torch.inverse(self.sigma())\n",
        "        mahalanobis = torch.sum((x - self.mu) @ Sigma_inv * (x - self.mu), dim=-1)\n",
        "        log_likelihood = -0.5 * (self.dim * torch.log(torch.tensor(2 * torch.pi)) + torch.log(Sigma_det) + mahalanobis)\n",
        "\n",
        "        return log_likelihood"
      ],
      "metadata": {
        "id": "HZtfalLY5TAF"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import logsumexp\n",
        "\n",
        "class GaussianMixtureModel(nn.Module):\n",
        "    def __init__(self, dim: int, K: int, prior: np.ndarray = None, mus = None, sigmas = None, use_softmax: bool = False):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.K = K\n",
        "        self.use_softmax = use_softmax\n",
        "        self.gaussians = nn.ModuleList([MultivariateGaussian(dim=dim) for _ in range(K)])\n",
        "        self.set_params(prior, mus, sigmas)\n",
        "        self.gaussians = nn.ModuleList(self.gaussians)\n",
        "\n",
        "    def sigma(self) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Computes the covariance matrix of the multivariate Gaussian distribution.\n",
        "        \"\"\"\n",
        "        Sigma = self.R @ self.R.T\n",
        "        return Sigma\n",
        "\n",
        "    def set_params(self, prior: np.ndarray = None, mus = None, sigmas = None) -> None:\n",
        "        \"\"\"\n",
        "        Sets the distribution parameter values. If unspecified, default values are used.\n",
        "        \"\"\"\n",
        "        if prior is None:\n",
        "            prior = np.ones(self.K) / self.K\n",
        "\n",
        "        if mus is None:\n",
        "            mus = [np.random.randn(self.dim) for _ in range(self.K)]\n",
        "\n",
        "        if sigmas is None:\n",
        "            sigmas = [np.eye(self.dim) for _ in range(self.K)]\n",
        "\n",
        "        self.prior_logits = nn.Parameter(torch.tensor(prior, dtype=torch.float32))\n",
        "\n",
        "        for i, gaussian in enumerate(self.gaussians):\n",
        "            sigma_i = sigmas[i]\n",
        "            if isinstance(sigma_i, torch.Tensor):\n",
        "                sigma_i = sigma_i.detach().numpy()\n",
        "\n",
        "            gaussian.mu.data = torch.tensor(mus[i], dtype=torch.float32)\n",
        "            gaussian.R.data = torch.tensor(np.linalg.cholesky(sigma_i), dtype=torch.float32)\n",
        "\n",
        "        return\n",
        "\n",
        "    def get_prior(self) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        If `use_softmax` attribute is True, apply softmax for prior normalization.\n",
        "        \"\"\"\n",
        "        if self.use_softmax:\n",
        "            prior = torch.softmax(self.prior_logits, dim=0)\n",
        "        else:\n",
        "            prior = self.prior_logits\n",
        "        return prior\n",
        "\n",
        "    def log_likelihood(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        no_of_points = x.shape[0]\n",
        "        log_probability = torch.zeros((no_of_points, self.K), dtype=torch.float32, device=x.device)\n",
        "\n",
        "        log_prior = torch.log(self.get_prior())\n",
        "\n",
        "        for k, gaussian in enumerate(self.gaussians):\n",
        "            log_probability[:, k] = gaussian.log_likelihood(x)\n",
        "\n",
        "        log_likelihood = torch.logsumexp(log_probability + log_prior, dim=1).sum()\n",
        "        return log_likelihood\n",
        "\n",
        "    def sample(self) -> torch.Tensor:\n",
        "        prior = self.get_prior()\n",
        "        random_comp = torch.multinomial(prior, num_samples=1).item()\n",
        "        sampled_vector = self.gaussians[random_comp].sample()\n",
        "        return sampled_vector\n",
        "\n",
        "    def get_gauss_params(self):\n",
        "        mus, sigmas = [], []\n",
        "        for gauss in self.gaussians:\n",
        "            mus.append(gauss.mu.detach().numpy())\n",
        "            sigmas.append(gauss.sigma().detach().numpy())\n",
        "        return mus, sigmas"
      ],
      "metadata": {
        "id": "Eu3LzDgL5Frf"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall the objective, which we want to maximize (1):\n",
        "\n",
        "$$\n",
        "\\operatorname{argmax}_{\\boldsymbol{\\theta}} p(\\mathbf{X} \\mid \\theta) = \\operatorname{argmax}_{\\boldsymbol{\\theta}}\\prod_{i=1}^N p(\\mathbf{x}_k \\mid \\theta) = \\operatorname{argmax}_{\\boldsymbol{\\theta}}\\prod_{i=1}^N \\Bigl(\\sum_{k=1}^K \\pi_k p_k(\\mathbf{x}_i \\mid \\mathbf{\\theta})\\Bigr) \\tag{1}\n",
        "$$\n",
        "\n",
        "- we start with an initialization $\\boldsymbol{\\theta}_0$ of the model parameters;\n",
        "- on the E-step, the cluster membership probabilities (often called responsibilities) of all datapoints are evaluated;\n",
        "- based on them, the MLE of the model parameters are updated."
      ],
      "metadata": {
        "id": "TxXLTplQ2TYf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Expectation step"
      ],
      "metadata": {
        "id": "h8SzZZdB3bj_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The responsibilities are calculated using the Bayes rule:\n",
        "\n",
        "$$r_{ik} = \\mathbb{P}(Z_i = k \\mid \\widehat{\\boldsymbol{\\theta}}, \\mathbf{x}_i) = \\frac{ \\hat{\\pi}_k p(\\mathbf{x}_i \\mid \\widehat{\\boldsymbol{\\theta}},Z_i = k)}{p(\\mathbf{x}_i \\mid \\widehat{\\boldsymbol{\\theta}})}\n",
        "  = \\frac{ \\hat{\\pi}_k p_k(\\mathbf{x}_i \\mid \\hat{\\boldsymbol{\\mu}}_k, \\widehat{\\Sigma}_k)}{p(\\mathbf{x}_i \\mid \\widehat{\\boldsymbol{\\theta}})}$$"
      ],
      "metadata": {
        "id": "FhpfDvE83eDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_responsibilities(model: nn.Module, data: torch.Tensor) -> torch.Tensor:\n",
        "    nominator = torch.stack([torch.log(model.get_prior()[i]) + model.gaussians[i].log_likelihood(data) for i in range(model.K)])\n",
        "    log_resp = nominator - torch.logsumexp(nominator, dim=0)\n",
        "    return log_resp"
      ],
      "metadata": {
        "id": "BsXXm7n810Vo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Maximization step"
      ],
      "metadata": {
        "id": "wm5HfppI4Nht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- the updated means $\\boldsymbol{\\mu}_k$ are simply the weighted averages of all data points $\\mathbf{x}_i$ with weights $r_{ik}$:\n",
        "  $$\n",
        "    \\hat{\\boldsymbol{\\mu}}_k = \\frac{\\sum_{i=1}^N r_{ik} \\mathbf{x}_i}{\\sum_{i=1}^N r_{ik}}\n",
        "  $$\n",
        "- the updated covariance matrices $\\widehat\\Sigma_k$ is the weighted analogue of the MLE estimate of the covariance:\n",
        "  $$\n",
        "    \\widehat\\Sigma_k = \\frac{\\sum_{i=1}^N r_{ik} (\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}}_k)(\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}}_k)^\\top}{\\sum_{i=1}^N r_{ik}}\n",
        "  $$\n",
        "- the updated mixing weights are\n",
        "  $$\n",
        "    \\hat \\pi_k = \\frac1N\\sum_{i=1}^N r_{ik}\n",
        "  $$"
      ],
      "metadata": {
        "id": "jKswa9Bq4mrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_params(model: nn.Module, log_resp: torch.Tensor, data: torch.Tensor):\n",
        "    resp = torch.exp(log_resp)\n",
        "    N, Dim = data.shape\n",
        "\n",
        "    prior = torch.sum(resp, dim=1) / N\n",
        "    mus = []\n",
        "    sigmas = []\n",
        "    for k in range(model.K):\n",
        "        mu = torch.sum(resp[k,:]*data.T, dim=1) / torch.sum(resp[k,:])\n",
        "        mus.append(mu)\n",
        "\n",
        "        diff = data - mu\n",
        "        sigma = torch.matmul((resp[k,:] * diff.T), diff) / torch.sum(resp[k, :])\n",
        "        sigmas.append(sigma)\n",
        "    model.set_params(prior=prior, mus=mus, sigmas=sigmas)"
      ],
      "metadata": {
        "id": "3CNSCtdh4PGU"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_gmm(data: np.ndarray, K: int) -> nn.Module:\n",
        "    N, D = data.shape\n",
        "    model = GaussianMixtureModel(dim=D, K=K, use_softmax=True)\n",
        "    return model\n",
        "\n",
        "def fit_gmm(model: nn.Module, data: np.ndarray, limit: int = 10000, delta: float = 1e-10):\n",
        "    model.train()\n",
        "    data = torch.tensor(data).to(torch.float32)\n",
        "    loss_history = []\n",
        "    for i in range(limit):\n",
        "        log_resp = log_responsibilities(model, data)\n",
        "        update_params(model, log_resp, data)\n",
        "        loss = -torch.mean(model.log_likelihood(data))\n",
        "        loss_history.append(loss.item())\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Iteration {i}, Loss: {loss.item()}\")\n",
        "        if i > 0 and abs(loss_history[-1] - loss_history[-2]) < delta:\n",
        "            break\n",
        "\n",
        "    return loss_history"
      ],
      "metadata": {
        "id": "wgWq9J4R4yfW"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting features from the images"
      ],
      "metadata": {
        "id": "o2ZUEMsB7LTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "wSIvpbUa7PvU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "id": "qhnmXp-s7W0M"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_paths = {\n",
        "    \"bombed\": \"data/bombed\",\n",
        "    \"not_bombed\": \"data/not bombed\"\n",
        "}"
      ],
      "metadata": {
        "id": "GvFbBf-X8P5p"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataloader(path):\n",
        "    dataset = datasets.ImageFolder(root=os.path.dirname(path), transform=transform)\n",
        "    class_name = os.path.basename(path)\n",
        "    class_idx = dataset.class_to_idx[class_name]\n",
        "    indices = [i for i, (_, y) in enumerate(dataset.samples) if y == class_idx]\n",
        "    subset = torch.utils.data.Subset(dataset, indices)\n",
        "    return DataLoader(subset, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "id": "sk6Ca0ju8mWc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bombed_loader = load_dataloader(data_paths[\"bombed\"])\n",
        "not_bombed_loader = load_dataloader(data_paths[\"not_bombed\"])"
      ],
      "metadata": {
        "id": "pvdISvlRI3lr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet = torchvision.models.resnet18(pretrained=True)\n",
        "resnet.fc = nn.Identity()\n",
        "resnet.eval()\n",
        "resnet.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Re07i8GzMD4p",
        "outputId": "017e7b47-3574-414a-bac2-3cb9084d8d69"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 169MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Identity()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(model, dataloader, device='cpu', max_batches=None):\n",
        "    features_list = []\n",
        "    labels_list = []\n",
        "    with torch.no_grad():\n",
        "        for i, (images, labels) in enumerate(dataloader):\n",
        "            images = images.to(device)\n",
        "            feats = model(images)\n",
        "            features_list.append(feats.cpu())\n",
        "            labels_list.append(labels)\n",
        "            if max_batches and i >= max_batches:\n",
        "                break\n",
        "    return torch.cat(features_list), torch.cat(labels_list)\n",
        "\n",
        "features, labels = extract_features(resnet, bombed_loader, device=device, max_batches=100)"
      ],
      "metadata": {
        "id": "pY-c0PUJMLkC"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gmm = initialize_gmm(features.numpy(), K=4)\n",
        "loss_history = fit_gmm(gmm, features.numpy())\n",
        "cur_mus, cur_sigmas = gmm.get_gauss_params()\n",
        "plot_gmm_data(features.numpy(), cur_mus, cur_sigmas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "3-k8ggy_MdwI",
        "outputId": "c67c17ac-692c-463a-d834-56ed24a1c874"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-60-468d2d46146b>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.prior_logits = nn.Parameter(torch.tensor(prior, dtype=torch.float32))\n",
            "<ipython-input-60-468d2d46146b>:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  gaussian.mu.data = torch.tensor(mus[i], dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LinAlgError",
          "evalue": "Matrix is not positive definite",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-c133e93e61ab>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_gmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_gmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcur_mus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_sigmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gauss_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplot_gmm_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_mus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_sigmas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-8e0e6977939e>\u001b[0m in \u001b[0;36mfit_gmm\u001b[0;34m(model, data, limit, delta)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mlog_resp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_responsibilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mupdate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_resp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-66-461cc8aded97>\u001b[0m in \u001b[0;36mupdate_params\u001b[0;34m(model, log_resp, data)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0msigmas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigmas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msigmas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-60-468d2d46146b>\u001b[0m in \u001b[0;36mset_params\u001b[0;34m(self, prior, mus, sigmas)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mgaussian\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mgaussian\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# ========== YOUR CODE ENDS HERE ========== #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/linalg/_linalg.py\u001b[0m in \u001b[0;36mcholesky\u001b[0;34m(a, upper)\u001b[0m\n\u001b[1;32m    837\u001b[0m     with errstate(call=_raise_linalgerror_nonposdef, invalid='call',\n\u001b[1;32m    838\u001b[0m                   over='ignore', divide='ignore', under='ignore'):\n\u001b[0;32m--> 839\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/linalg/_linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_nonposdef\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Matrix is not positive definite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_eigenvalues_nonconvergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLinAlgError\u001b[0m: Matrix is not positive definite"
          ]
        }
      ]
    }
  ]
}